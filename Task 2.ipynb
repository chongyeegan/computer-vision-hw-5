{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To install OpenCV3: type \"conda install -c menpo opencv3=3.1.0\" on terminal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import scipy.ndimage.filters as filters\n",
    "from skimage.feature import canny\n",
    "\n",
    "from skimage.feature import corner_harris, corner_peaks\n",
    "from sklearn import linear_model\n",
    "from skimage.transform import warp, AffineTransform\n",
    "from skimage.measure import ransac\n",
    "from skimage.util.shape import view_as_windows\n",
    "from numpy.linalg import norm as normalize\n",
    "import cv2\n",
    "\n",
    "if not os.path.exists(\"Q2\"):\n",
    "    os.makedirs(\"Q2\")\n",
    "    \n",
    "if not os.path.exists(\"Q2/Roadsign\"):\n",
    "    os.makedirs(\"Q2/Roadsign\")\n",
    "    \n",
    "if not os.path.exists(\"Q2/Starbucks\"):\n",
    "    os.makedirs(\"Q2/Starbucks\")\n",
    "    \n",
    "if not os.path.exists(\"Q2/Superman\"):\n",
    "    os.makedirs(\"Q2/Superman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turns .gif files to .png, and returns the resultant .png filepath. \n",
    "# reference: http://stackoverflow.com/questions/6689380/how-to-change-gif-file-to-png-file-using-python-pil\n",
    "def gifToPng(fn):\n",
    "    files = glob.glob(fn) \n",
    "\n",
    "    for imageFile in files:\n",
    "        filepath,filename = os.path.split(imageFile)\n",
    "        filterame,exts = os.path.splitext(filename)\n",
    "        print \"Processing: \" + imageFile,filterame\n",
    "        im = Image.open(imageFile)\n",
    "        tempPath = 'Images/Q2/Starbucks/'+filterame+'.png'\n",
    "        im.save( tempPath,'PNG')\n",
    "    \n",
    "        return tempPath\n",
    "    \n",
    "def createPyramids(image, pyr_number):\n",
    "    \n",
    "    G = image.copy()\n",
    "    pyrGauss = [G]\n",
    "    \n",
    "    #creating Gaussian Pyramid\n",
    "    for i in range(1,pyr_number):\n",
    "        G = cv2.pyrDown(G)\n",
    "        pyrGauss.append(G)\n",
    "        \n",
    "    return pyrGauss\n",
    "\n",
    "def detectObject(img, temp):\n",
    "    \n",
    "    MIN_MATCH_COUNT = 1\n",
    "    imageOri = cv2.imread(img,0)\n",
    "    \n",
    "    template = cv2.imread(temp,0)\n",
    "    image = cv2.imread(img,0)\n",
    "    \n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(template,None)\n",
    "    kp2, des2 = sift.detectAndCompute(image,None)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    #flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    #matches = flann.knnMatch(des1,des2,k=2)\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "            \n",
    "    # If there are more than \"MIN_MATCH_COUNT\" matches, find their perspective transform\n",
    "    if len(good)>MIN_MATCH_COUNT:\n",
    "        src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "        if mask != None:\n",
    "            matchesMask = mask.ravel().tolist()\n",
    "\n",
    "            h,w = template.shape\n",
    "            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "            dst = cv2.perspectiveTransform(pts,M)\n",
    "    \n",
    "            imageOri = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)\n",
    "            imageOri = cv2.polylines(imageOri,[np.int32(dst)],True,(0,255,0),2)\n",
    "    \n",
    "    else:\n",
    "        print \"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT)\n",
    "        matchesMask = None\n",
    "    \n",
    "    ## Uncomment to see bounding box with matching points\n",
    "    '''\n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                       singlePointColor = None,\n",
    "                       matchesMask = matchesMask, # draw only inliers\n",
    "                       flags = 2)\n",
    "    img3 = cv2.drawMatches(template,kp1,image,kp2,good,None,**draw_params)\n",
    "    '''\n",
    "    plt.imshow(imageOri, cmap=\"gray\") \n",
    "    fn = img.split('/')[-2] + '/' + img.split('/')[-1].split('.')[0] + '_detected.jpg'\n",
    "    plt.savefig('Q2/' + fn, dpi=200)\n",
    "    print fn\n",
    "    plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongyeegan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:63: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roadsign/image1_detected.jpg\n",
      "Roadsign/image2_detected.jpg\n",
      "Roadsign/image3_detected.jpg\n",
      "Roadsign/image4_detected.jpg\n",
      "Roadsign/image5_detected.jpg\n",
      "Roadsign/image6_detected.jpg\n",
      "Roadsign/image7_detected.jpg\n",
      "Roadsign/image8_detected.jpg\n",
      "Roadsign/image9_detected.jpg\n",
      "Roadsign/image10_detected.jpg\n",
      "Roadsign/image11_detected.jpg\n",
      "Roadsign/image12_detected.jpg\n",
      "Roadsign/lollipop-man_detected.jpg\n",
      "Processing: ./Images/Q2/Starbucks/template.gif template\n",
      "Starbucks/image1_detected.jpg\n",
      "Starbucks/image2_detected.jpg\n",
      "Starbucks/image2_detected.jpg\n",
      "Starbucks/image3_detected.jpg\n",
      "Starbucks/image4_detected.jpg\n",
      "Starbucks/image5_detected.jpg\n",
      "Starbucks/image6_detected.jpg\n",
      "Starbucks/image7_detected.jpg\n",
      "Starbucks/image8_detected.jpg\n",
      "Starbucks/image9_detected.jpg\n",
      "Starbucks/image10_detected.jpg\n",
      "Starbucks/image11_detected.jpg\n",
      "Starbucks/image12_detected.jpg\n",
      "Starbucks/image12_detected.jpg\n",
      "Starbucks/image13_detected.jpg\n",
      "Starbucks/image14_detected.jpg\n",
      "Starbucks/image15_detected.jpg\n",
      "Starbucks/image16_detected.jpg\n",
      "Superman/image1_detected.jpg\n",
      "Superman/image2_detected.jpg\n",
      "Superman/image3_detected.jpg\n",
      "Superman/image4_detected.jpg\n",
      "Superman/image5_detected.jpg\n",
      "Superman/image6_detected.jpg\n",
      "Superman/image7_detected.jpg\n",
      "Superman/image8_detected.jpg\n",
      "Superman/image9_detected.jpg\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Detect Roadsigns\n",
    "    imgPath = './Images/Q2/Roadsign/image'\n",
    "    tempPath = './Images/Q2/Roadsign/template.jpg'\n",
    "    #tempPath = './Images/Q2/Roadsign/template.jpg'\n",
    "    #tempPath = cv2.imread('./Images/Q2/Roadsign/template.jpg',0)\n",
    "    \n",
    "    for i in range(1, 13):\n",
    "        #if i == 6 or i == 7 or i == 8 or i == 12:\n",
    "        #    tempPath = createPyramids(tempPath,5)[1]\n",
    "        detectObject(imgPath + str(i) + '.jpg', tempPath)\n",
    "    detectObject('./Images/Q2/Roadsign/lollipop-man.jpg', tempPath)\n",
    "    \n",
    "    # Detect Starbucks\n",
    "    imgPath = './Images/Q2/Starbucks/image'\n",
    "    tempPath = gifToPng('./Images/Q2/Starbucks/template.gif')\n",
    "    for i in range(1, 17):\n",
    "        if i == 2 or i == 12:\n",
    "            detectObject(imgPath + str(i) + '.jpg', tempPath)\n",
    "        detectObject(imgPath + str(i) + '.jpg', tempPath)\n",
    "        \n",
    "    # Detect Superman\n",
    "    imgPath = './Images/Q2/Superman/image'\n",
    "    tempPath = './Images/Q2/Superman/template.jpg'\n",
    "    for i in range(1, 10):\n",
    "        detectObject(imgPath + str(i) + '.jpg', tempPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turns .gif files to .png, and returns the resultant .png filepath. \n",
    "# reference: http://stackoverflow.com/questions/6689380/how-to-change-gif-file-to-png-file-using-python-pil\n",
    "def gifToPng(fn):\n",
    "    files = glob.glob(fn) \n",
    "\n",
    "    for imageFile in files:\n",
    "        filepath,filename = os.path.split(imageFile)\n",
    "        filterame,exts = os.path.splitext(filename)\n",
    "        print \"Processing: \" + imageFile,filterame\n",
    "        im = Image.open(imageFile)\n",
    "        tempPath = 'Images/Q2/Starbucks/'+filterame+'.png'\n",
    "        im.save( tempPath,'PNG')\n",
    "    \n",
    "        return tempPath\n",
    "    \n",
    "def createPyramids(image, pyr_number):\n",
    "    \n",
    "    G = image.copy()\n",
    "    pyrGauss = [G]\n",
    "    \n",
    "    #creating Gaussian Pyramid\n",
    "    for i in range(1,pyr_number):\n",
    "        G = cv2.pyrDown(G)\n",
    "        pyrGauss.append(G)\n",
    "        \n",
    "    return pyrGauss\n",
    "\n",
    "def getKP(image, template):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(template,None)\n",
    "    kp2, des2 = sift.detectAndCompute(image,None)\n",
    "\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "            \n",
    "    # If there are more than \"MIN_MATCH_COUNT\" matches, find their perspective transform\n",
    "    src_pts = [ kp1[m.queryIdx].pt for m in good ]\n",
    "    dst_pts = [ kp2[m.trainIdx].pt for m in good ]\n",
    "    \n",
    "    return src_pts, dst_pts\n",
    "\n",
    "def detectObject(img, temp):\n",
    "    \n",
    "    MIN_MATCH_COUNT = 1\n",
    "    imageOri = cv2.imread(img,0)\n",
    "    \n",
    "    template = cv2.imread(temp,0)\n",
    "    #template = cv2.Canny(template, 30, 200)\n",
    "    \n",
    "    image = cv2.imread(img,0)\n",
    "\n",
    "    imageDenoise = cv2.imread(img)\n",
    "    imageDenoise = cv2.fastNlMeansDenoisingColored(imageDenoise,None,10,10,7,21)\n",
    "    imageDenoise = cv2.cvtColor(imageDenoise, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    imageContrast = cv2.imread(img,0)\n",
    "    clahe = cv2.createCLAHE(clipLimit=0.5, tileGridSize=(8,8))\n",
    "    imageContrast = clahe.apply(imageContrast)\n",
    "    \n",
    "    imageDenoiseAndContrast = clahe.apply(imageDenoise)\n",
    "        \n",
    "    templateBlur = cv2.imread(temp,0)\n",
    "    templateBlur = cv2.GaussianBlur(templateBlur,(5,5),0)\n",
    "    \n",
    "    templateDenoise = cv2.imread(temp)\n",
    "    templateDenoise = cv2.fastNlMeansDenoisingColored(templateDenoise,None,10,10,7,21)\n",
    "    templateDenoise = cv2.cvtColor(templateDenoise, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    templateDenoiseAndBlur = cv2.GaussianBlur(templateDenoise,(5,5),0)\n",
    "    \n",
    "    templateTranslate = cv2.imread(temp,0)\n",
    "    rows,cols = templateTranslate.shape\n",
    "    M = np.float32([[1,0,0],[0,1,-100]])\n",
    "    templateTranslate = cv2.warpAffine(templateTranslate,M,(cols,rows))\n",
    "    \n",
    "    templateBlurAndTranslate = cv2.warpAffine(templateBlur,M,(cols,rows))\n",
    "    \n",
    "    src_pts,dst_pts = getKP(image, templateBlur)\n",
    "    s,d = getKP(image, templateDenoise)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(image, templateDenoiseAndBlur)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(image, templateTranslate)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(image, templateBlurAndTranslate)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageDenoise, templateBlur)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageDenoise, templateDenoiseAndBlur)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageDenoise, templateDenoise)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageDenoise, templateTranslate)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageDenoise, templateBlurAndTranslate)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageContrast, templateBlur)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageContrast, templateDenoise)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageContrast, templateDenoiseAndBlur)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageContrast, templateTranslate)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageContrast, templateBlurAndTranslate)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageDenoiseAndContrast, templateBlur)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageDenoiseAndContrast, templateDenoise)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageDenoiseAndContrast, templateDenoiseAndBlur)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageDenoiseAndContrast, templateTranslate)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    s,d = getKP(imageDenoiseAndContrast, templateBlurAndTranslate)\n",
    "    src_pts.extend(s)\n",
    "    dst_pts.extend(d)\n",
    "    \n",
    "    src_pts = np.float32(src_pts).reshape(-1,1,2)\n",
    "    dst_pts = np.float32(dst_pts).reshape(-1,1,2)\n",
    "    \n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    if mask != None:\n",
    "        matchesMask = mask.ravel().tolist()\n",
    "\n",
    "        h,w = template.shape\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "        dst = cv2.perspectiveTransform(pts,M)\n",
    "    \n",
    "        imageOri = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)\n",
    "        imageOri = cv2.polylines(imageOri,[np.int32(dst)],True,(0,255,0),2)\n",
    "    \n",
    "    \n",
    "    plt.imshow(imageOri, cmap=\"gray\") \n",
    "    fn = img.split('/')[-2] + '/' + img.split('/')[-1].split('.')[0] + '_detected.jpg'\n",
    "    plt.savefig('Q2/' + fn, dpi=200)\n",
    "    print fn\n",
    "    plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongyeegan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:149: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roadsign/image1_detected.jpg\n",
      "Roadsign/image2_detected.jpg\n",
      "Roadsign/image3_detected.jpg\n",
      "Roadsign/image4_detected.jpg\n",
      "Roadsign/image5_detected.jpg\n",
      "Roadsign/image6_detected.jpg\n",
      "Roadsign/image7_detected.jpg\n",
      "Roadsign/image8_detected.jpg\n",
      "Roadsign/image9_detected.jpg\n",
      "Roadsign/image10_detected.jpg\n",
      "Roadsign/image11_detected.jpg\n",
      "Roadsign/image12_detected.jpg\n",
      "Roadsign/lollipop-man_detected.jpg\n",
      "Processing: ./Images/Q2/Starbucks/template.gif template\n",
      "Starbucks/image1_detected.jpg\n",
      "Starbucks/image2_detected.jpg\n",
      "Starbucks/image2_detected.jpg\n",
      "Starbucks/image3_detected.jpg\n",
      "Starbucks/image4_detected.jpg\n",
      "Starbucks/image5_detected.jpg\n",
      "Starbucks/image6_detected.jpg\n",
      "Starbucks/image7_detected.jpg\n",
      "Starbucks/image8_detected.jpg\n",
      "Starbucks/image9_detected.jpg\n",
      "Starbucks/image10_detected.jpg\n",
      "Starbucks/image11_detected.jpg\n",
      "Starbucks/image12_detected.jpg\n",
      "Starbucks/image12_detected.jpg\n",
      "Starbucks/image13_detected.jpg\n",
      "Starbucks/image14_detected.jpg\n",
      "Starbucks/image15_detected.jpg\n",
      "Starbucks/image16_detected.jpg\n",
      "Superman/image1_detected.jpg\n",
      "Superman/image2_detected.jpg\n",
      "Superman/image3_detected.jpg\n",
      "Superman/image4_detected.jpg\n",
      "Superman/image5_detected.jpg\n",
      "Superman/image6_detected.jpg\n",
      "Superman/image7_detected.jpg\n",
      "Superman/image8_detected.jpg\n",
      "Superman/image9_detected.jpg\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Detect Roadsigns\n",
    "    imgPath = './Images/Q2/Roadsign/image'\n",
    "    tempPath = './Images/Q2/Roadsign/template.jpg'\n",
    "    #tempPath = './Images/Q2/Roadsign/template.jpg'\n",
    "    #tempPath = cv2.imread('./Images/Q2/Roadsign/template.jpg',0)\n",
    "    \n",
    "    for i in range(1, 13):\n",
    "        #if i == 6 or i == 7 or i == 8 or i == 12:\n",
    "        #    tempPath = createPyramids(tempPath,5)[1]\n",
    "        detectObject(imgPath + str(i) + '.jpg', tempPath)\n",
    "    detectObject('./Images/Q2/Roadsign/lollipop-man.jpg', tempPath)\n",
    "    \n",
    "    # Detect Starbucks\n",
    "    imgPath = './Images/Q2/Starbucks/image'\n",
    "    tempPath = gifToPng('./Images/Q2/Starbucks/template.gif')\n",
    "    for i in range(1, 17):\n",
    "        if i == 2 or i == 12:\n",
    "            detectObject(imgPath + str(i) + '.jpg', tempPath)\n",
    "        detectObject(imgPath + str(i) + '.jpg', tempPath)\n",
    "        \n",
    "    # Detect Superman\n",
    "    imgPath = './Images/Q2/Superman/image'\n",
    "    tempPath = './Images/Q2/Superman/template.jpg'\n",
    "    for i in range(1, 10):\n",
    "        detectObject(imgPath + str(i) + '.jpg', tempPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix (not used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming Image before matching (slightly improved but need individual modification; not robust)\n",
    "Tried gaussian pyramid ver of template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turns .gif files to .png, and returns the resultant .png filepath. \n",
    "# reference: http://stackoverflow.com/questions/6689380/how-to-change-gif-file-to-png-file-using-python-pil\n",
    "def gifToPng(fn):\n",
    "    files = glob.glob(fn) \n",
    "\n",
    "    for imageFile in files:\n",
    "        filepath,filename = os.path.split(imageFile)\n",
    "        filterame,exts = os.path.splitext(filename)\n",
    "        print \"Processing: \" + imageFile,filterame\n",
    "        im = Image.open(imageFile)\n",
    "        tempPath = 'Images/Q2/Starbucks/'+filterame+'.png'\n",
    "        im.save( tempPath,'PNG')\n",
    "    \n",
    "        return tempPath\n",
    "    \n",
    "def createPyramids(image, pyr_number):\n",
    "    \n",
    "    G = image.copy()\n",
    "    pyrGauss = [G]\n",
    "    \n",
    "    #creating Gaussian Pyramid\n",
    "    for i in range(1,pyr_number):\n",
    "        G = cv2.pyrDown(G)\n",
    "        pyrGauss.append(G)\n",
    "        \n",
    "    return pyrGauss\n",
    "\n",
    "def detectObject(img, temp, denoise):\n",
    "    \n",
    "    MIN_MATCH_COUNT = 1\n",
    "    imageOri = cv2.imread(img,0)\n",
    "    \n",
    "    template = cv2.imread(temp,0)\n",
    "    #template = cv2.Canny(template, 30, 200)\n",
    "    if (denoise == True):\n",
    "        image = cv2.imread(img)\n",
    "        image = cv2.fastNlMeansDenoisingColored(image,None,10,10,7,21)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        clahe = cv2.createCLAHE(clipLimit=0.5, tileGridSize=(8,8))\n",
    "        image = clahe.apply(image)\n",
    "    else:    \n",
    "        image = cv2.imread(img,0) \n",
    "        #image = cv2.Canny(image, 30, 200)\n",
    "        #image = cv2.bilateralFilter(image,9,75,75)\n",
    "        clahe = cv2.createCLAHE(clipLimit=0.5, tileGridSize=(8,8))\n",
    "        image = clahe.apply(image)\n",
    "    \n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(template,None)\n",
    "    kp2, des2 = sift.detectAndCompute(image,None)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    #flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    #matches = flann.knnMatch(des1,des2,k=2)\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "            \n",
    "    # If there are more than \"MIN_MATCH_COUNT\" matches, find their perspective transform\n",
    "    if len(good)>MIN_MATCH_COUNT:\n",
    "        src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "        if mask != None:\n",
    "            matchesMask = mask.ravel().tolist()\n",
    "\n",
    "            h,w = template.shape\n",
    "            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "            dst = cv2.perspectiveTransform(pts,M)\n",
    "    \n",
    "            imageOri = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)\n",
    "            imageOri = cv2.polylines(imageOri,[np.int32(dst)],True,(0,255,0),2)\n",
    "    \n",
    "    else:\n",
    "        print \"Not enough matches are found - %d/%d\" % (len(good),MIN_MATCH_COUNT)\n",
    "        matchesMask = None\n",
    "    \n",
    "    ## Uncomment to see bounding box with matching points\n",
    "    '''\n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                       singlePointColor = None,\n",
    "                       matchesMask = matchesMask, # draw only inliers\n",
    "                       flags = 2)\n",
    "    img3 = cv2.drawMatches(template,kp1,image,kp2,good,None,**draw_params)\n",
    "    '''\n",
    "    plt.imshow(imageOri, cmap=\"gray\") \n",
    "    fn = img.split('/')[-2] + '/' + img.split('/')[-1].split('.')[0] + '_detected.jpg'\n",
    "    plt.savefig('Q2/' + fn, dpi=200)\n",
    "    print fn\n",
    "    plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding box drawn have bad results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    \n",
    "def detectObject(img, temp):\n",
    "    \n",
    "    template = cv2.imread(temp,0)\n",
    "    image = cv2.imread(img,0)\n",
    "    \n",
    "    # Find Matched Feature Points\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    #sift = cv2.SIFT()\n",
    "    \n",
    "    # find keypoints and descriptors\n",
    "    kp1, des1 = sift.detectAndCompute(template,None)\n",
    "    kp2, des2 = sift.detectAndCompute(image,None)\n",
    "    \n",
    "    # FLANN parameters\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks=50)\n",
    "\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    #flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "    #matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "    good = []\n",
    "\n",
    "    # ratio test as per Lowe's paper. Store good matches and matched points\n",
    "    for i,(m,n) in enumerate(matches):\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "            \n",
    "    # If there are more than 10 matches, find their perspective transform\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "\n",
    "    h,w = image.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "    \n",
    "    image = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)\n",
    "    image = cv2.polylines(image,[np.int32(dst)],True,(0,255,0),3)\n",
    "    \n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                       singlePointColor = None,\n",
    "                       matchesMask = matchesMask, # draw only inliers\n",
    "                       flags = 2)\n",
    "\n",
    "    img3 = cv2.drawMatches(template,kp1,image,kp2,good,None,**draw_params)\n",
    "\n",
    "    plt.imshow(img3, cmap = 'gray') \n",
    "    fn = img.split('/')[-2] + '/' + img.split('/')[-1].split('.')[0] + '_detected.jpg'\n",
    "    plt.savefig('Q2/' + fn, dpi=200)\n",
    "    print fn\n",
    "    plt.close()\n",
    "    \n",
    "    #else:\n",
    "    #    print \"Not enough matches are found in %s - %d/%d\" % (img,len(good),MIN_MATCH_COUNT)\n",
    "    #    matchesMask = None\n",
    "      \n",
    "    # Draw Matches    \n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 2)\n",
    "\n",
    "    matchedImage = cv2.drawMatches(image,kp1,template,kp2,good,None,**draw_params)\n",
    "    plt.imshow(matchedImage, 'gray') \n",
    "    fn = img.split('/')[-2] + '/' + img.split('/')[-1].split('.')[0] + '_matched.jpg'\n",
    "    plt.savefig('Q2/' + fn, dpi=200)\n",
    "    plt.close()\n",
    "    '''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./Images/Q2/Starbucks/template.gif template\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'ravel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f85a009a57ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtempPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./Images/Q2/Superman/template.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdetectObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgPath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-19ac206a1170>\u001b[0m in \u001b[0;36mdetectObject\u001b[0;34m(img, temp)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindHomography\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRANSAC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmatchesMask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ravel'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
